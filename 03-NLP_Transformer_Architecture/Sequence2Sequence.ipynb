{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Architecture\n",
    "\n",
    "In this notebook we are going to dive into the Attention mechanism in Sequence2Sequence models to understand how they work and can be implemented in PyTorch.\n",
    "\n",
    "This is needed so we can later, in another Notebook, understand the Transformer Model and apply it correctly.\n",
    "For reading up on the Attention mechanism, I propose [this blog article](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) which explains the concept in a visual yet detailed way.\n",
    "Additionally, the PyTorch Tutorials also give a good explanation on the subject: [Link](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "# import sys\n",
    "# sys.path.insert(0, '.') # searches this directory too -> ensures that imports work fine.\n",
    "\n",
    "# from modules.tatoeba import TatoebaDataset\n",
    "from modules.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.tatoeba import TatoebaDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/deu-eng/deu.txt', sep=\"\\t\", names=['en', 'de', 'license'])\n",
    "ttds = TatoebaDataset(data, max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> s='go .'\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of NoneType",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7e73a452fda8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0men\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mttds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Yasin/01-Projects/21-06-03-Path_to_PyTorch/pytorch-path/03-NLP_Transformer_Architecture/modules/tatoeba.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# transform to tensors, .view(-1,1) just ensures that we have 1-d vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{v0=}\\n{v1=}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of NoneType"
     ]
    }
   ],
   "source": [
    "en, de = ttds[0]"
   ]
  },
  {
   "source": [
    "## The Encoder Architecture\n",
    "\n",
    "The encoder will take our \"word-ids\" (remember the Tokenizer) and encode them into a a vector: The hidden representation. Lateron our decoder network will use this hidden representation to generate the needed output.\n",
    "\n",
    "Let's first have a look at an important component for many NLP-tasks: The `Embedding`-Layer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> s=array(['keep it .', 'behalt es !'], dtype=object)\nSample Sentence: Keep it. -> keep it .\ntensor([[3153],\n        [3889],\n        [3952],\n        [   1]])\nShape: torch.Size([4, 1])\n\nEmdedding Output: \ntensor([[[ 0.8552,  1.4405, -0.2495, -0.1888,  0.5496, -0.0926, -0.9891,\n          -0.4037, -0.1124, -1.5778, -0.3150, -0.2862,  1.1753,  0.2026,\n           0.1486, -0.3349]],\n\n        [[ 0.6217, -0.7885, -1.2634, -1.0896,  0.3202,  0.3956,  0.7785,\n           2.0619,  0.7042,  0.8439, -0.0092,  0.5039, -0.0093,  0.6477,\n           1.4604, -1.4763]],\n\n        [[-0.5785,  0.1268,  2.0047, -1.0455,  0.4273,  1.5664, -0.1494,\n          -1.2897, -1.0611, -0.8506,  0.7665,  1.4411, -0.2000, -1.2499,\n          -0.3104,  0.4136]],\n\n        [[ 1.2055, -0.6775, -0.9319, -1.1549,  1.1883,  0.3843,  1.3659,\n           1.3357, -1.6840,  0.3782,  1.0045, -0.8648,  1.2299,  1.7879,\n           0.6085,  0.3778]]], grad_fn=<EmbeddingBackward>) \nShape: torch.Size([4, 1, 16])\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = ttds.vocab_size()[0] # number of tokens in English\n",
    "hidden_dimension = 16\n",
    "query_index = 250\n",
    "emb = nn.Embedding(vocabulary_size, hidden_dimension)\n",
    "\n",
    "sample_sentence_eng, sample_sentence_deu = ttds[query_index]\n",
    "print(f\"Sample Sentence: {ttds.df.iloc[query_index,0]} -> {ttds.df.iloc[query_index, 3]}\\n{sample_sentence_eng}\\nShape: {sample_sentence_eng.shape}\")\n",
    "\n",
    "output = emb(sample_sentence_eng)\n",
    "print(f\"\\nEmdedding Output: \\n{output} \\nShape: {output.shape}\")"
   ]
  },
  {
   "source": [
    "What we can see here is, that the variable length input, in our case with 3 tokens, gets transformed into 3 vectors of size 16 (= hidden_dimension).\n",
    "\n",
    "This 16-dimensional representation is not arbitrary or fixed, though. During the training of our model it will learn to perfectly encode our different tokens.\n",
    "(If we have time, we can look into the resulting embeddings. Spoiler: Similar tokens get similar vectors. How 'similar' is defined, remains open here.)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### PyTorch Implementation of the Encoder Architecture"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from torch import nn"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_dimension):\n",
    "        \"\"\"\n",
    "        param: input_size: The size of our vocabulary.\n",
    "        param: hidden_dimension: The size of our latent space (-> complexity our model can memorize)\n",
    "        \"\"\"\n",
    "\n",
    "        # call init method of super-class `nn.Module` to initialize import things\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dimension = hidden_dimension # store for later\n",
    "\n",
    "        # create Embedding layer\n",
    "        self.embedding_layer = nn.Embedding(input_size, hidden_dimension)\n",
    "\n",
    "        # create a GRU cell / layer\n",
    "        self.recurrent_layer = nn.GRU(hidden_dimension, hidden_dimension)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        \"\"\"\n",
    "        This method computes the propagation through our encoder.\n",
    "        Note: We have two inputs, because in Recurrent Neural Networks\n",
    "        we look at the previous timestep as well.\n",
    "        \"\"\"\n",
    "\n",
    "        embedding = self.embedding_layer(x).view(1,1, -1)\n",
    "        x_new, h_new = self.recurrent_layer(embedding, h)\n",
    "\n",
    "        return x_new, h_new"
   ]
  },
  {
   "source": [
    "## The Decoder Architecture\n",
    "\n",
    "After we have explored the Encoder, the Decoder isn't as surprising now.\n",
    "It basically performs the same step, but backwards:\n",
    "\n",
    "Given a vector of a certain dimensionality, it tries to produce value that, when used with our embedding, result in our tokens again. Of course one important question is: How does it reprocude tokens?\n",
    "\n",
    "Easy answer: The last layer of our Decoder is a simpe fully-connected Linear layer with <vocab_size> output vector.\n",
    "That means that for the right token, for example \"hello\" -> 2143, it will light up the resulting vector at the right place (ex. 2143th position)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_dimension, output_size):\n",
    "        \"\"\"\n",
    "        param: output_size: The size of our target language's vocabulary.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "\n",
    "        # create embedding layer\n",
    "        self.embedding_layer = nn.Embedding(output_size, hidden_dimension)\n",
    "\n",
    "        # another recurrent layer (so we look at the last input again)\n",
    "        self.gru_layer = nn.GRU(hidden_dimension, hidden_dimension)\n",
    "\n",
    "        # our output layer for the token\n",
    "        self.out_layer = nn.Linear(hidden_dimension, output_size)\n",
    "\n",
    "        # the activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        \"\"\"\n",
    "        Performs the forward computation in our Decoder.\n",
    "        \"\"\"\n",
    "        x_new = self.embedding_layer(x).view(1, 1, -1)\n",
    "        x_new = F.relu(x_new)\n",
    "        x_new, hidden = self.gru_layer(x_new, h)\n",
    "        x_new = self.softmax(self.out_layer(x_new[0]))\n",
    "        return x_new, hidden"
   ]
  },
  {
   "source": [
    "## Test: One forward pass through our networks."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ">> s=array(['keep it .', 'behalt es !'], dtype=object)\nSample: Keep it.\n"
     ]
    }
   ],
   "source": [
    "eng_size, de_size = ttds.vocab_size()\n",
    "hidden_dimension = 16\n",
    "query_index = 250\n",
    "\n",
    "encoder = Encoder(eng_size, hidden_dimension)\n",
    "decoder = Decoder(hidden_dimension, de_size)\n",
    "\n",
    "sample_sentence_eng, sample_sentence_deu = ttds[query_index]\n",
    "sentence_length_eng = sample_sentence_eng.shape[0] # number of tokens\n",
    "print(f\"Sample: {ttds.df.iloc[query_index, 0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0870,  0.2710, -0.4172,  0.2653, -0.3467,  0.2383, -0.2118,\n",
       "           -0.5551,  0.3691, -0.3275, -0.4928,  0.1960, -0.1272,  0.0613,\n",
       "            0.0291,  0.1308]]], grad_fn=<StackBackward>),\n",
       " torch.Size([1, 1, 16]))"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "h_enc = torch.zeros(1,1, hidden_dimension) #  initial hidden vector is just zeros\n",
    "\n",
    "for token_idx in range(sentence_length_eng):\n",
    "    token = sample_sentence_eng[token_idx]\n",
    "    x_enc, h_enc = encoder(token, h_enc)\n",
    "\n",
    "x_enc, x_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0)\ntensor(0)\ntensor(0)\ntensor(0)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "h_dec = h_enc\n",
    "x_dec = torch.Tensor([ttds.t1['SOS']]).type(torch.long)\n",
    "\n",
    "sentence_length_deu = sample_sentence_deu.shape[0]\n",
    "\n",
    "for token_idx in range(sentence_length_deu):\n",
    "    output, h_dec = decoder(x_dec, h_dec)\n",
    "    x_dec = sample_sentence_deu[token_idx]\n",
    "    print(torch.argmax(x_dec))\n",
    "x_dec.shape"
   ]
  },
  {
   "source": [
    "## Putting it all together - for now\n",
    "\n",
    "Although you may already see points where we can improve our model, I'd like to take a short break and test things as they are right now.\n",
    "\n",
    "This means putting together our training pipeline to really \"learn\" from the samples we provide the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 15\n",
    "HIDDEN_DIMENSION = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_iteration(input_x, target, encoder, decoder, encoder_optim, decoder_optim, loss_function):\n",
    "\n",
    "    h_enc = torch.zeros\n",
    "\n",
    "    # reset optimizers and loss value\n",
    "    encoder_optim.zero_grad()\n",
    "    decoder_optim.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    # ENCODER: iterate over input tokens\n",
    "    outputs = torch.zeros(MAX_LENGTH, encoder.hidden_dimension)\n",
    "    for idx in range(input_x.shape[0]):\n",
    "        token = input_x[idx]\n",
    "        x_enc, h_enc = encoder(token, h_enc)\n",
    "        outputs[idx] = x_enc\n",
    "\n",
    "    # DECODER\n",
    "    x_dec = torch.Tensor([ttds.t1['SOS']])\n",
    "    # init hidden representation of decoder with resulting hidden representation of encoder\n",
    "    h_dec = h_enc \n",
    "    for idx in (target.shape[0]):\n",
    "        x_dec, h_dec = decoder(x_dec, h_dec)\n",
    "        loss += loss_function(x_dec, target[idx]) # accumulate loss for decoder\n",
    "        x_dec = target[idx] # teacher-forcing (explained in cell)\n",
    "\n",
    "    # OPTIMIZATION\n",
    "    loss.backward() # run accumulated loss through decoder backwards to encoder\n",
    "    encoder_optim.step()\n",
    "    decoder_optim.step()\n",
    "\n",
    "    # average loss per token\n",
    "    return loss.item() / target.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, decoder, dataset, iterations=1000):\n",
    "\n",
    "    # create optimizers as well as loss function\n",
    "    encoder_optim = optim.SGD(encoder.parameters(), lr=0.01)\n",
    "    decoder_optim = optim.SGD(decoder.parameters(), lr=0.01)\n",
    "    loss_function = nn.NLLLoss()\n",
    "\n",
    "    num_tokens = len(dataset)\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        query_idx = torch.randint(low=0, high=num_tokens, size=(1,)) # choose random training pair\n",
    "        print(f\"Querying sentence at {query_idx.item()}\")\n",
    "        s0, s1 = dataset.df.loc[query_idx, [dataset.langs[0] + '_clean', dataset.langs[1] + '_clean']]\n",
    "        print(f\"{s0=}{s1=}\")\n",
    "        source_tensor, target_tensor = dataset[query_idx]\n",
    "\n",
    "        loss = training_iteration(source_tensor, target_tensor, \n",
    "                encoder, decoder, encoder_optim, decoder_optim, loss_function\n",
    "        )\n",
    "        print(f\"Iteration {iteration:03} | Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del TatoebaDataset\n",
    "from modules.tatoeba import TatoebaDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TatoebaDataset(data, max_length=MAX_LENGTH, debug=True)\n",
    "eng_size, deu_size = dataset.vocab_size()\n",
    "encoder = Encoder(eng_size, HIDDEN_DIMENSION)\n",
    "decoder = Decoder(HIDDEN_DIMENSION, deu_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Querying sentence at 11020\ns0='en_clean's1='de_clean'\n                    en                       de  \\\n11020  We're famished.  Wir sind am Verhungern.   \n\n                                                 license          en_clean  \\\n11020  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  we re famished .   \n\n                       de_clean  \n11020  wir sind am verhungern .  \n>> s=array([['we re famished .', 'wir sind am verhungern .']], dtype=object)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7ef7a4c76366>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-8c7d3f7babbc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, dataset, iterations)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0ms0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlangs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_clean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlangs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_clean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{s0=}{s1=}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0msource_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         loss = training_iteration(source_tensor, target_tensor, \n",
      "\u001b[0;32m~/Yasin/01-Projects/21-06-03-Path_to_PyTorch/pytorch-path/03-NLP_Transformer_Architecture/modules/tatoeba.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\">> {s=}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0ms0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0ms1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# vectorize sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "train(encoder, decoder, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "source": [
    "torch.Tensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[46036, 45500, 29177, 46952, 1]"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "v0 = [dataset.t0[s] for s in s0.split(\" \") if s != \"\"] + [dataset.t0['EOS']]\n",
    "v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[46036, 45500, 29177, 46952]"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "[dataset.t0[s] for s in s0.split(\" \") if s != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "s0='come get it .' and s1='komm und hol es dir .'\n"
     ]
    }
   ],
   "source": [
    "s0, s1 = dataset.df.loc[2146, [dataset.langs[0] + '_clean', dataset.langs[1] + '_clean']].values\n",
    "print(f\"{s0=} and {s1=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['come closer .', 'komm naher .'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "dataset.df.iloc[2143, 3:5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [['we re famished .', 'wir sind am verhungern .']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('pytorch-path': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "c133cf9be758c35ebeb78e620c4ff535f87a940c93836c640d7b902017586f6b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}