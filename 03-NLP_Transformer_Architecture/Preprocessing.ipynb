{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "In this notebook we are going to perform the work needed for using the data with our models.\n",
    "This includes the following steps:\n",
    "\n",
    "1. Analyzing the data\n",
    "2. Preprocessing the data\n",
    "3. Writing a DataLoader to ensure it can be used with our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We are going to use a classic dataset: The [Tab-delimited Bilingual Sentence Pairs](http://www.manythings.org/anki/) from the [Tatoeba project](https://tatoeba.org/en).\n",
    "\n",
    "Since I am a native German speaker (and therefore can check the results the fastest in), we are going to use the Eng-Deu dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>de</th>\n",
       "      <th>license</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>We try.</td>\n",
       "      <td>Wir versuchen es.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>We won.</td>\n",
       "      <td>Wir haben gewonnen.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>It's his.</td>\n",
       "      <td>Es ist seins.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>It's hot.</td>\n",
       "      <td>Es ist heiß.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>They were busy.</td>\n",
       "      <td>Sie waren beschäftigt.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>They were dead.</td>\n",
       "      <td>Sie waren tot.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    en                      de  \\\n",
       "100            We try.       Wir versuchen es.   \n",
       "101            We won.     Wir haben gewonnen.   \n",
       "500          It's his.           Es ist seins.   \n",
       "501          It's hot.            Es ist heiß.   \n",
       "10000  They were busy.  Sie waren beschäftigt.   \n",
       "10001  They were dead.          Sie waren tot.   \n",
       "\n",
       "                                                 license  \n",
       "100    CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "101    CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "500    CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "501    CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "10000  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "10001  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/deu-eng/deu.txt', sep='\\t', names=['en', 'de', 'license'])\n",
    "data.iloc[np.r_[100:102, 500:502, 10000:10002]] # display some parts of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "Instead of implementing various methods for splitting words, removing non-ascii chars, etc. we are just letting nltk do all the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'How', 'are', 'you', 'doing', '?', '✅']\n"
     ]
    }
   ],
   "source": [
    "test =  \"Hello! How are you doing?✅\"\n",
    "print(word_tokenize(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(strip_accents='ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentences = data.loc[:, ['en']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(strip_accents='ascii')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(en_sentences.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 16334)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform(['Hello! How are you doing?']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6895"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_.get('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6895.],\n",
       "        [ 7174.],\n",
       "        [  992.],\n",
       "        [16285.],\n",
       "        [ 4537.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"hello how are you doing\"\n",
    "torch.Tensor([cv.vocabulary_[s] for s in sentence.split(\" \")]).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_['hoi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x16334 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform(['hoi'])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TatoebaDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df:pd.DataFrame, max_length=10, langs=('en', 'de'), debug=False):\n",
    "        self.debug = debug\n",
    "        self.df = df.copy(deep=True)\n",
    "        \n",
    "        # filter by max length\n",
    "        self.df = self.df.loc[df[langs[0]].str.len() <= max_length, :]\n",
    "        \n",
    "        # clean all strings\n",
    "        self.df.loc[:, langs[0]] = self.df.loc[:, langs[0]].apply(self.clean_string)\n",
    "        self.df.loc[:, langs[1]] = self.df[langs[1]].apply(self.clean_string)\n",
    "        \n",
    "        # create CountVectorizers for both languages\n",
    "        token_pattern = r\"(?u)\\b\\w\\w+\\b|!|\\?|.\" # what to match as tokens \n",
    "        # self.tokenizer = RegexpTokenizer(r\"(?u)\\b\\w\\w+\\b|!|\\?|.|\\S+\") # will be used later\n",
    "        self.cv0 = CountVectorizer(strip_accents='ascii', token_pattern=token_pattern).fit(self.df[langs[0]].values.ravel())\n",
    "        self.cv1 = CountVectorizer(strip_accents='ascii', token_pattern=token_pattern).fit(self.df[langs[1]].values.ravel())\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # get sentences in both languages from line <index>\n",
    "        s0, s1 = self.df.iloc[index, 0:2]\n",
    "        \n",
    "        # vectorize sentences\n",
    "        if self.debug: print(f\" >> Eng: {s0}, Deu {s1}\")\n",
    "        if self.debug: print(f\" >>Tokenized: {s0.split(' ')}\")\n",
    "        v0 = [self.cv0.vocabulary_[s.lower()] for s in s0.split(\" \") if s != \"\"]\n",
    "        v1 = [self.cv1.vocabulary_[s.lower()] for s in s1.split(\" \") if s != \"\"]\n",
    "        \n",
    "        # transform to tensors\n",
    "        t0 = torch.tensor(v0).view(-1,1)\n",
    "        t1 = torch.tensor(v1).view(-1,1)\n",
    "        \n",
    "        return t0, t1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_string(string):\n",
    "        \"\"\"Cleans a given string so it can be vectorized\"\"\"\n",
    "        string = re.sub(r\"([.!?])\", r\" \\1\", string)\n",
    "        string = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", string) # what to keep in the strings\n",
    "        \n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttds = TatoebaDataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 250: Eng: Keep it . | Deu: Behalt es !\n"
     ]
    }
   ],
   "source": [
    "idx = 250\n",
    "print(f\"Index: {idx}: Eng: {data.iloc[idx, 0]} | Deu: {data.iloc[idx, 1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-path",
   "language": "python",
   "name": "pytorch-path"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
