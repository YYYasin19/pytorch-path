{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "In this notebook we are going to perform the work needed for using the data with our models.\n",
    "This includes the following steps:\n",
    "\n",
    "1. Analyzing the data\n",
    "2. Preprocessing the data\n",
    "3. Writing a DataLoader to ensure it can be used with our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Dataset\n",
    "\n",
    "We are going to use a classic dataset: The [Tab-delimited Bilingual Sentence Pairs](http://www.manythings.org/anki/) from the [Tatoeba project](https://tatoeba.org/en).\n",
    "\n",
    "Since I am a native German speaker (and therefore can check the results the fastest in), we are going to use the Eng-Deu dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    en                      de  \\\n",
       "100            We try.       Wir versuchen es.   \n",
       "101            We won.     Wir haben gewonnen.   \n",
       "500          It's his.           Es ist seins.   \n",
       "501          It's hot.            Es ist heiß.   \n",
       "10000  They were busy.  Sie waren beschäftigt.   \n",
       "10001  They were dead.          Sie waren tot.   \n",
       "\n",
       "                                                 license  \n",
       "100    CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "101    CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "500    CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "501    CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "10000  CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "10001  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>de</th>\n      <th>license</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>100</th>\n      <td>We try.</td>\n      <td>Wir versuchen es.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n    </tr>\n    <tr>\n      <th>101</th>\n      <td>We won.</td>\n      <td>Wir haben gewonnen.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n    </tr>\n    <tr>\n      <th>500</th>\n      <td>It's his.</td>\n      <td>Es ist seins.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>It's hot.</td>\n      <td>Es ist heiß.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n    </tr>\n    <tr>\n      <th>10000</th>\n      <td>They were busy.</td>\n      <td>Sie waren beschäftigt.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n    </tr>\n    <tr>\n      <th>10001</th>\n      <td>They were dead.</td>\n      <td>Sie waren tot.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "data = pd.read_csv('data/deu-eng/deu.txt', sep='\\t', names=['en', 'de', 'license'])\n",
    "data.iloc[np.r_[100:102, 500:502, 10000:10002]] # display some parts of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "In this section we want to preprocess our data. More specifically, we are going to tokenize our vocabulary (= all words from our corpus) so that we have a unique token for each possible identifier.\n",
    "\n",
    "~Instead of implementing various methods for splitting words, removing non-ascii chars, etc. we are just using nltk's `word_tokenize` and sklearn's `CountVectorizer` to perform the tasks~\n",
    "\n",
    "We are implementing our own `Tokenizer`-class! This gives us more flexibility on which strings to include and how to retrieve data lateron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt') # -> uncomment this cell if you haven't downloaded the stopwords yet!\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word_vector = {\n",
    "            'sos': 0, \n",
    "            'eos': 1\n",
    "        }\n",
    "        self.n_words = 2\n",
    "        \n",
    "    def add_string(self, string:str):\n",
    "        \"\"\"adds this string to the word_vector\"\"\"\n",
    "        \n",
    "        tokens = self.tokenize_string(string)\n",
    "        for token in tokens:\n",
    "            self.word_vector[token] = self.n_words\n",
    "            self.n_words += 1\n",
    "            \n",
    "    def add_data(self, data:pd.Series):\n",
    "        strings = data.values.ravel()\n",
    "        for s in strings:\n",
    "            self.add_string(s)\n",
    "            \n",
    "    def tokenize_string(self, string):\n",
    "        clean_s = self.clean_string(string)\n",
    "        \n",
    "        # just split \n",
    "        return clean_s.split(\" \")\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def to_ascii(string):\n",
    "        return ''.join(\n",
    "            character for character in unicodedata.normalize('NFD', string) if unicodedata.category(character) != 'Mn'\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_string(string):\n",
    "        s = Tokenizer.to_ascii(string.lower().strip())\n",
    "        s = re.sub(r\"([.!?])\", r\" \\1\", s) # adds space before certain punctuation marks\n",
    "        s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "        return s\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # clean the index too\n",
    "        c_index = Tokenizer.clean_string(index).strip()\n",
    "        if c_index in self.word_vector:\n",
    "            return self.word_vector[c_index]\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch's Dataset-class\n",
    "\n",
    "Since the main goal of our project is to learn PyTorch more, we are going to use the `Dataset`-class provided by the PyTorch Framework.\n",
    "\n",
    "Taking the intial dataframe as parameter, this class / object allows us to preprocess each string the same way and access our training samples in a fast and clean way.\n",
    "\n",
    "Bonus points: It works really well with PyTorch's `DataLoader` which we'll see in a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TatoebaDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df:pd.DataFrame, max_length=None, langs=('en', 'de'), debug=False):\n",
    "        self.debug = debug\n",
    "        self.df = df.copy(deep=True)\n",
    "        self.langs = langs\n",
    "        \n",
    "        # filter by max length\n",
    "        self.df = self.df.loc[df[langs[0]].str.len() <= max_length, :]\n",
    "        \n",
    "        # clean all strings -> add 'clean'-versions of language data!\n",
    "        self.df.loc[:, langs[0] + '_clean'] = self.df.loc[:, langs[0]].apply(Tokenizer.clean_string)\n",
    "        self.df.loc[:, langs[1] + '_clean'] = self.df[langs[1]].apply(Tokenizer.clean_string)\n",
    "        \n",
    "        # create the tokenizers for both languages\n",
    "        self.t0 = Tokenizer()\n",
    "        self.t0.add_data(self.df[langs[0] + '_clean'])\n",
    "        self.t1 = Tokenizer()\n",
    "        self.t1.add_data(self.df[langs[1] + '_clean'])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index > self.__len__():\n",
    "            return None\n",
    "        \n",
    "        # get sentences in both languages from line <index>\n",
    "        s0, s1 = self.df.loc[index, [self.langs[0] + '_clean', self.langs[1] + '_clean']]\n",
    "        \n",
    "        # vectorize sentences\n",
    "        if self.debug: print(f\" >> {self.langs[0]}: {s0}, {self.langs[1]} {s1}\")\n",
    "        if self.debug: print(f\" >> Tokenized: {s0.split(' ')}\")\n",
    "        \n",
    "        v0 = [self.t0[s] for s in s0.split(\" \") if s != \"\"] + self.t0['EOS']\n",
    "        v1 = [self.t1[s] for s in s1.split(\" \") if s != \"\"] + self.t1['EOS']\n",
    "        \n",
    "        # transform to tensors, .view(-1,1) just ensures that we have 1-d vectors\n",
    "        t0 = torch.tensor(v0).view(-1,1)\n",
    "        t1 = torch.tensor(v1).view(-1,1)\n",
    "        \n",
    "        return t0, t1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "        pass\n",
    "\n",
    "    def vocab_size(self):\n",
    "        return self.t0.n_words, self.t1.n_words\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index: 450:\nEng: I use it.\nDeu: Ich benutze es.\n--------------------------------------------------------------------------------\nEng: tensor([[2913],\n        [1835],\n        [3889],\n        [3952]])\nDeu: tensor([[3997],\n        [1516],\n        [4306],\n        [4438]])\n"
     ]
    }
   ],
   "source": [
    "# SHOWCASE\n",
    "\n",
    "# init the object with the raw-data we have from our csv\n",
    "ttds = TatoebaDataset(data, max_length=10) # limit to 10 words\n",
    "\n",
    "# the sentences at the following index are:\n",
    "idx = 450\n",
    "print(f\"Index: {idx}:\\nEng: {data.iloc[idx, 0]}\\nDeu: {data.iloc[idx, 1]}\\n\" + \"-\"*80)\n",
    "\n",
    "# the dataset-object directly returns tensors that we can use for training\n",
    "eng_tensor, deu_tensor = ttds[idx]\n",
    "\n",
    "print(f\"Eng: {eng_tensor}\\nDeu: {deu_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This already concludes Part 1 where we implement the first part of our pipeline: Actually getting the data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-path",
   "language": "python",
   "name": "pytorch-path"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}